{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Config Data\n",
    "\n",
    "Load in configuration data, which will dictate the behavior of the scraper.  The login information will be used to log into untappd.  The search_terms is only required if the url_file does not exist.  If it exists, the search scraping will not occur.  \n",
    "\n",
    "**Sample Config File**\n",
    "\n",
    "Open \"untappd_sample.cfg\" for a sample configuration file.  Add a username and password.  If you make a copy and name it untappd.cfg, git will ignore it and your password will not be checked in. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "config_path = 'untappd.cfg'\n",
    "\n",
    "with open(config_path) as rdr:\n",
    "    config = json.load(rdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create web driver using the scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import untappd_scraper\n",
    "from untappd_scraper import ScraperType\n",
    "\n",
    "browser = untappd_scraper.create_driver(config, headless=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Beer URLs to Scrape\n",
    "\n",
    "If the url_file in the config exists, we'll use that.  Otherwise we'll use the search terms to begin scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs Found: 3733\n"
     ]
    }
   ],
   "source": [
    "url_file = config['scraping']['url_file']\n",
    "\n",
    "if not os.path.exists(url_file):\n",
    "    urls = []\n",
    "    \n",
    "    ## Create search term scraper\n",
    "    search_term_scraper = untappd_scraper.create_scraper(ScraperType.SEARCH, browser)\n",
    "    \n",
    "    for search_term in config['scraping']['search_terms']:\n",
    "        urls.extend(search_term_scraper.scrape_search_term(search_term))\n",
    "    \n",
    "    urls = list(set(urls))\n",
    "    untappd_scraper.write_pkl(url_file, urls)\n",
    "    \n",
    "else:\n",
    "    urls = untappd_scraper.read_pkl(url_file)\n",
    "    \n",
    "print('URLs Found:', len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of URLS before filter: 3733\n",
      "Number of URLS after filter:  2259\n",
      "1) https://untappd.com/beer/566994 found 290 reviews\n",
      "2) https://untappd.com/beer/2717978 found 32 reviews\n",
      "3) https://untappd.com/beer/68933 found 290 reviews\n",
      "4) https://untappd.com/beer/2216258 found 290 reviews\n",
      "5) https://untappd.com/beer/1991200 found 14 reviews\n",
      "6) https://untappd.com/beer/2561113 found 20 reviews\n",
      "7) https://untappd.com/beer/2800334 found 190 reviews\n",
      "8) https://untappd.com/beer/2943303 found 66 reviews\n",
      "9) https://untappd.com/beer/397375 found 290 reviews\n",
      "10) https://untappd.com/beer/2694348 found 77 reviews\n",
      "11) https://untappd.com/beer/581938 found 265 reviews\n",
      "12) https://untappd.com/beer/1796100 found 290 reviews\n",
      "13) https://untappd.com/beer/1400728 found 290 reviews\n",
      "14) https://untappd.com/beer/2108251 found 103 reviews\n",
      "15) https://untappd.com/beer/2738595 found 290 reviews\n",
      "16) https://untappd.com/beer/802219 found 265 reviews\n",
      "17) https://untappd.com/beer/2978738 found 290 reviews\n",
      "18) https://untappd.com/beer/1456 found 265 reviews\n",
      "19) https://untappd.com/beer/998766 found 290 reviews\n",
      "20) https://untappd.com/beer/2977867 found 115 reviews\n",
      "21) https://untappd.com/beer/16630 found 290 reviews\n",
      "22) https://untappd.com/beer/1936683 found 290 reviews\n",
      "23) https://untappd.com/beer/26490 found 95 reviews\n",
      "24) https://untappd.com/beer/2892115 found 290 reviews\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import feather\n",
    "\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "## Identify all existing urls, and remove them from our url list\n",
    "df = pd.concat([feather.read_dataframe(file) for file in glob('../data/beer-info*.feather')])\n",
    "df.head()\n",
    "\n",
    "existing_ids = set(df['id'])\n",
    "\n",
    "print(\"Number of URLS before filter:\", len(urls))\n",
    "urls = [url for url in urls if int(url.split('/')[-1]) not in existing_ids]\n",
    "print(\"Number of URLS after filter: \", len(urls))\n",
    "\n",
    "beer_scraper = untappd_scraper.create_scraper(ScraperType.BEER, browser)\n",
    "\n",
    "beer_results = []\n",
    "review_results = []\n",
    "\n",
    "for url in urls:\n",
    "    \n",
    "    beers, reviews = beer_scraper.scrape_beer(url)\n",
    "    beer_results.append(beers)\n",
    "    review_results.extend(reviews)\n",
    "    \n",
    "    print(f\"{len(beer_results)}) {url} found {len(reviews)} reviews\")\n",
    "    ## Every 25 beers write out the beer info and reviews\n",
    "    if len(beer_results) >= 50:\n",
    "        print(\"Clearing\")\n",
    "        file_id = str(uuid.uuid4())\n",
    "        \n",
    "        # Write beer info\n",
    "        feather.write_dataframe(untappd_scraper.create_beer_df(beer_results), f'../data/beer-info_{file_id}.feather')\n",
    "        pd.DataFrame(beer_results).to_json(f'../data/beer-info_{file_id}.json', orient='records')\n",
    "\n",
    "        # Write user reviews\n",
    "        feather.write_dataframe(untappd_scraper.create_reviews_df(review_results), f'../data/reviews_{file_id}.feather')\n",
    "        pd.DataFrame(review_results).to_json(f'../data/reviews_{file_id}.json', orient='records')\n",
    "        \n",
    "        beer_results = []\n",
    "        review_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: \n",
    "\n",
    " - Done? Clean the data, write out JSON and feather with same uuid\n",
    "Add scraping a user\n",
    "Add optional flag to NOT scraper reviews from a beer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
